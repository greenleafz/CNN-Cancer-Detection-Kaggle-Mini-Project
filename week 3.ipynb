{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\r\n",
    "\r\n",
    "The Histopathologic Cancer Detection competition, hosted on Kaggle, presents a critical and challenging task in the medical imaging domain. Participants are tasked with developing an algorithm capable of identifying metastatic cancer from small image patches extracted from larger digital pathology scans. This problem is framed as a binary image classification task, where the objective is to determine whether a given image patch contains at least one pixel of tumor tissue.\r\n",
    "\r\n",
    "Metastasis detection is a pivotal step in cancer diagnosis, influencing treatment decisions and patient outcomes. The early detection of metastatic tissue in lymph node sections can significantly impact the clinical management of cancer, making this competition not only a challenge for machine learning enthusi withwith profound implications automated cancer screeningcare.\r\n",
    "\r\n",
    "The dataset for this competition is derived from the PatchCamelyon (PCam) benchmark dataset. The PCam dataset is notable for its clinicalance,  relevcomacking the complex task of metastasis detection into a straightforward binary classific challs, akin to well-known datasets like CIFAR-10 and MNIST. However, the competition dataset has been curated to remove duplicates present in the original PCam dataset, ensuriniquetionue set of images for model training stingmodels.\r\n",
    "\r\n",
    "In the following  we sectionschar explore the dataset through exploratory data analysis, discuss the methodology for model building and training, present the results of our experiments, and conclude with a discussion on the potential clinical implications and directions for future research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\green\\AppData\\Local\\Temp\\ipykernel_15692\\2536968432.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "C:\\Users\\green\\anaconda3\\envs\\tf\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Data manipulation and numerical libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Machine learning and neural network libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Evaluation and progress tracking\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Dataset\r\n",
    "In this section, we begin our exploratory data analysis (EDA) by loading the dataset containing image labels. The dataset consists of a CSV file with two columns: id, which corresponds to the filename of the image, and label, indicating whether the image contains tumor tissue (1) or not (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         id  label\n",
      "0  f38a6374c348f90b587e046aac6079959adf3835      0\n",
      "1  c18f2d887b7ae4f6742ee445113fa1aef383ed77      1\n",
      "2  755db6279dae599ebb4d39a9123cce439965282d      0\n",
      "3  bc3f0c64fb968ff4a8bd33af6971ecae77c75e08      0\n",
      "4  068aba587a4950175d04c680d38943fd488d6a9d      0\n"
     ]
    }
   ],
   "source": [
    "labels_df = pd.read_csv('./train_labels.csv')\n",
    "print(labels_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the Label Distribution\r\n",
    "After loading the dataset, it' simportanl to examine the distribution of labels to understand the balance between the classes. A balanced dataset can significantlyimprovet the performance of our machine learning models. Here, we visualize the distribution of the binary labels in our dataset, where 1 indicates the presence of tumor tissue in the image, and 0 signifies its absence.Base on this check, we can see that although there is a slight imbalance, the counts are reletively similar, making imbalance compensation unnecessary. ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x='label', data=labels_df)\n",
    "plt.title('Distribution of Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and Data Augmentation\r\n",
    "Before training our model ,we nedl to preprocess the dataset to ensure it is in a format suitable for feeding into a convolutional neural network. This includes normalizing the image pixel values and setting up data generators for efficiently managing image data during training and validation. Additionally, we adjust the labels and filenames to match the expected format for our model training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating ImageDataGenerator...\n",
      "ImageDataGenerator Complete \n",
      "Modifying Labels...\n",
      "Label Modification Complete \n",
      "Creating train_generator...\n",
      "Found 176020 validated image filenames belonging to 2 classes.\n",
      "train_generator Complete \n",
      "Creating validation_generator...\n",
      "Found 44005 validated image filenames belonging to 2 classes.\n",
      "validation_generator Complete\n"
     ]
    }
   ],
   "source": [
    "# Initializing an ImageDataGenerator with normalization for preprocessing\n",
    "print('Creating ImageDataGenerator...')\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,  # Normalizing the pixel values to [0, 1]\n",
    "    validation_split=0.2  # Allocating 20% of the data for validation\n",
    ")\n",
    "print('ImageDataGenerator Complete', '\\nModifying Labels...')\n",
    "\n",
    "# Adjusting the 'label' column to string format for compatibility with Keras\n",
    "labels_df['label'] = labels_df['label'].astype(str)\n",
    "\n",
    "# Appending '.tif' extension to file IDs to match actual image filenames\n",
    "labels_df['id'] = labels_df['id'].apply(lambda x: f\"{x}.tif\")\n",
    "print('Label Modification Complete', '\\nCreating train_generator...')\n",
    "\n",
    "# Setting up the training data generator\n",
    "train_generator = datagen.flow_from_dataframe(\n",
    "    dataframe=labels_df,\n",
    "    directory='./train/',  # Directory where training images are stored\n",
    "    x_col='id',  # Column in DataFrame containing image filenames\n",
    "    y_col='label',  # Column in DataFrame containing labels\n",
    "    target_size=(96, 96),  # Resizing images to 96x96 pixels\n",
    "    batch_size=32,  # Number of images to process in a batch\n",
    "    class_mode='binary',  # Binary labels for binary classification task\n",
    "    subset='training'  # Indicating this generator is for training data\n",
    ")\n",
    "print('train_generator Complete', '\\nCreating validation_generator...')\n",
    "\n",
    "# Setting up the validation data generator\n",
    "validation_generator = datagen.flow_from_dataframe(\n",
    "    dataframe=labels_df,\n",
    "    directory='./train/',  # Directory where validation images are stored\n",
    "    x_col='id',  # Column in DataFrame containing image filenames\n",
    "    y_col='label',  # Column in DataFrame containing labels\n",
    "    target_size=(96, 96),  # Resizing images to 96x96 pixels for consistency\n",
    "    batch_size=32,  # Number of images to process in a batch\n",
    "    class_mode='binary',  # Binary labels for the classification task\n",
    "    subset='validation'  # Indicating this generator is for validation data\n",
    ")\n",
    "print('validation_generator Complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparative Analysis of Model Architectures\n",
    "\n",
    "In our pursuit to develop an effective algorithm for identifying metastatic cancer in histopathologic scan images, we employ a comparative analysis strategy. This approach involves defining, training, and evaluating three distinct convolutional neural network (CNN) architectures. Our primary metric for comparison is the Area Under the Receiver Operating Characteristic Curve (AUC_ROC), aligning with the competition's evaluation criteria. Each model is designed with a unique structure to explore different aspects of CNN capabilities in the context of medical image analysis.\n",
    "\n",
    "### Model 1: Custom CNN Architecture\n",
    "\n",
    "The first model is a custom CNN designed to capture hierarchical feature representations of the images. This architecture consists of successive convolutional layers, each followed by max-pooling to reduce spatial dimensions and increase the field of view. The network concludes with dense layers, introducing a high-level feature interpretation before making a binary classification.\n",
    "\n",
    "### Model 2: Enhanced CNN with Batch Normalization\n",
    "\n",
    "Our second model builds on the first by incorporating batch normalization after each convolutional layer. Batch normalization aims to stabilize learning by normalizing the input to each activation function, potentially improving training speed and overall model performance. This model retains the structure of alternating convolutional and max-pooling layers, with the addition of normalization to foster more efficient learning dynamics.\n",
    "\n",
    "### Model 3: Transfer Learning with VGG16\n",
    "\n",
    "The third model leverages transfer learning, utilizing the VGG16 network pre-trained on ImageNet as a feature extractor. The top layers of VGG16 are replaced with a custom classifier tailored to our binary classification task. This approach allows us to harness the rich feature representations learned by VGG16 on a diverse set of images, applying them to the specific challenge of metastasis detection. The base model's weights are frozen to preserve the learned features, with only the newly added layers being trained.\n",
    "\n",
    "### Compilation and Training Strategy\n",
    "\n",
    "Each model is compiled with the Adam optimizer and binary cross-entropy loss function. We track both accuracy and AUC to monitor performance, emphasizing the AUC_ROC score for its relevance to the competition's evaluation metric. The training process involves fitting each model to the data while employing early stopping and model checkpointing to mitigate overfitting and ensure we capture the best version of each model based on validation AUC.\n",
    "\n",
    "This comparative analysis allows us to evaluate the efficacy of different architectural decisions and training strategies in the context of cancer detection. By examining the models' AUC_ROC scores, we aim to identify the most promising approach for further optimization and deployment in clinical settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom CNN architecture\n",
    "model = Sequential([\n",
    "    Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(96, 96, 3)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(filters=128, kernel_size=(3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Flatten(),\n",
    "    Dense(units=512, activation='relu'),\n",
    "    Dropout(rate=0.25),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model with Adam optimizer and binary crossentropy loss, including accuracy and AUC as metrics\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'AUC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an enhanced CNN model using batch normalization for improved stability during training\n",
    "model_2 = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(96, 96, 3)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model with Adam optimizer, binary crossentropy loss, and track accuracy and AUC\n",
    "model_2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilize VGG16 pre-trained model for transfer learning, freezing its weights to leverage learned features\n",
    "base_model = VGG16(input_shape=(96, 96, 3), include_top=False, weights='imagenet')\n",
    "base_model.trainable = False  # Prevent the base model from being updated during training\n",
    "\n",
    "# Define a custom model built on top of VGG16\n",
    "model_3 = Sequential([\n",
    "    base_model,  # Incorporate the VGG16 base model\n",
    "    layers.Flatten(),  # Flatten the output to prepare for the dense layer\n",
    "    layers.Dense(512, activation='relu'),  # Dense layer for feature interpretation\n",
    "    layers.Dropout(0.5),  # Dropout layer for regularization\n",
    "    layers.Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "])\n",
    "\n",
    "# Compile the model with Adam optimizer, binary crossentropy as loss, and track accuracy and AUC\n",
    "model_3.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'AUC'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each model is tailored to explore different CNN capabilities and strategies, from custom architectures to enhanced models with batch normalization and the application of transfer learning. The models are compiled with a focus on binary classification, optimizing for both accuracy and the area under the ROC curve (AUC), aligning with the competition's evaluation metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Process and Model Evaluation\n",
    "\n",
    "To ensure our models are trained effectively and to mitigate the risk of overfitting, we employ two key callback functions during the training process: **Early Stopping** and **Model Checkpoint**. **Early Stopping** monitors the validation AUC (Area Under the Curve) and halts training if there's no improvement in model performance after a specified number of epochs, thereby preventing overfitting and unnecessary computation. **Model Checkpoint** saves the best version of the model based on the validation AUC, ensuring we retain the model's best state for future evaluation and inference.  This systematic approach allows us to compare the models directly based on their performance and identify the most effective architecture for detecting metastatic cancer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model_1...\n",
      "Epoch 1/100\n",
      "5501/5501 [==============================] - ETA: 0s - loss: 0.4093 - accuracy: 0.8159 - auc: 0.8893\n",
      "Epoch 1: val_auc improved from -inf to 0.91697, saving model to best_model_1_01.h5\n",
      "5501/5501 [==============================] - 530s 95ms/step - loss: 0.4093 - accuracy: 0.8159 - auc: 0.8893 - val_loss: 0.3918 - val_accuracy: 0.8328 - val_auc: 0.9170\n",
      "Epoch 2/100\n",
      "5499/5501 [============================>.] - ETA: 0s - loss: 0.3068 - accuracy: 0.8726 - auc: 0.9388\n",
      "Epoch 2: val_auc improved from 0.91697 to 0.94943, saving model to best_model_1_02.h5\n",
      "5501/5501 [==============================] - 146s 27ms/step - loss: 0.3067 - accuracy: 0.8726 - auc: 0.9388 - val_loss: 0.3219 - val_accuracy: 0.8646 - val_auc: 0.9494\n",
      "Epoch 3/100\n",
      "5500/5501 [============================>.] - ETA: 0s - loss: 0.2693 - accuracy: 0.8900 - auc: 0.9530\n",
      "Epoch 3: val_auc improved from 0.94943 to 0.95674, saving model to best_model_1_03.h5\n",
      "5501/5501 [==============================] - 147s 27ms/step - loss: 0.2693 - accuracy: 0.8900 - auc: 0.9530 - val_loss: 0.2601 - val_accuracy: 0.8971 - val_auc: 0.9567\n",
      "Epoch 4/100\n",
      "5501/5501 [==============================] - ETA: 0s - loss: 0.2455 - accuracy: 0.9011 - auc: 0.9607\n",
      "Epoch 4: val_auc improved from 0.95674 to 0.96333, saving model to best_model_1_04.h5\n",
      "5501/5501 [==============================] - 144s 26ms/step - loss: 0.2455 - accuracy: 0.9011 - auc: 0.9607 - val_loss: 0.2368 - val_accuracy: 0.9056 - val_auc: 0.9633\n",
      "Epoch 5/100\n",
      "5500/5501 [============================>.] - ETA: 0s - loss: 0.2254 - accuracy: 0.9102 - auc: 0.9668\n",
      "Epoch 5: val_auc did not improve from 0.96333\n",
      "5501/5501 [==============================] - 145s 26ms/step - loss: 0.2254 - accuracy: 0.9102 - auc: 0.9667 - val_loss: 0.2430 - val_accuracy: 0.9063 - val_auc: 0.9612\n",
      "Epoch 6/100\n",
      "5501/5501 [==============================] - ETA: 0s - loss: 0.2056 - accuracy: 0.9194 - auc: 0.9721\n",
      "Epoch 6: val_auc did not improve from 0.96333\n",
      "5501/5501 [==============================] - 145s 26ms/step - loss: 0.2056 - accuracy: 0.9194 - auc: 0.9721 - val_loss: 0.2404 - val_accuracy: 0.9083 - val_auc: 0.9633\n",
      "Epoch 7/100\n",
      "5501/5501 [==============================] - ETA: 0s - loss: 0.1877 - accuracy: 0.9278 - auc: 0.9766\n",
      "Epoch 7: val_auc did not improve from 0.96333\n",
      "5501/5501 [==============================] - 137s 25ms/step - loss: 0.1877 - accuracy: 0.9278 - auc: 0.9766 - val_loss: 0.2437 - val_accuracy: 0.9087 - val_auc: 0.9629\n",
      "Epoch 8/100\n",
      "5500/5501 [============================>.] - ETA: 0s - loss: 0.1671 - accuracy: 0.9361 - auc: 0.9813\n",
      "Epoch 8: val_auc did not improve from 0.96333\n",
      "5501/5501 [==============================] - 140s 26ms/step - loss: 0.1671 - accuracy: 0.9361 - auc: 0.9812 - val_loss: 0.2785 - val_accuracy: 0.8935 - val_auc: 0.9565\n",
      "Epoch 9/100\n",
      "5500/5501 [============================>.] - ETA: 0s - loss: 0.1419 - accuracy: 0.9466 - auc: 0.9863\n",
      "Epoch 9: val_auc did not improve from 0.96333\n",
      "5501/5501 [==============================] - 140s 26ms/step - loss: 0.1419 - accuracy: 0.9466 - auc: 0.9863 - val_loss: 0.2608 - val_accuracy: 0.9076 - val_auc: 0.9620\n",
      "Epoch 10/100\n",
      "5499/5501 [============================>.] - ETA: 0s - loss: 0.1216 - accuracy: 0.9553 - auc: 0.9896\n",
      "Epoch 10: val_auc did not improve from 0.96333\n",
      "5501/5501 [==============================] - 140s 26ms/step - loss: 0.1216 - accuracy: 0.9553 - auc: 0.9896 - val_loss: 0.2834 - val_accuracy: 0.9015 - val_auc: 0.9580\n",
      "Epoch 11/100\n",
      "5500/5501 [============================>.] - ETA: 0s - loss: 0.1029 - accuracy: 0.9618 - auc: 0.9924\n",
      "Epoch 11: val_auc did not improve from 0.96333\n",
      "5501/5501 [==============================] - 141s 26ms/step - loss: 0.1029 - accuracy: 0.9618 - auc: 0.9924 - val_loss: 0.3349 - val_accuracy: 0.8960 - val_auc: 0.9514\n",
      "Epoch 12/100\n",
      "5499/5501 [============================>.] - ETA: 0s - loss: 0.0883 - accuracy: 0.9677 - auc: 0.9942\n",
      "Epoch 12: val_auc did not improve from 0.96333\n",
      "5501/5501 [==============================] - 141s 26ms/step - loss: 0.0883 - accuracy: 0.9677 - auc: 0.9942 - val_loss: 0.3681 - val_accuracy: 0.9011 - val_auc: 0.9509\n",
      "Epoch 13/100\n",
      "5501/5501 [==============================] - ETA: 0s - loss: 0.0892 - accuracy: 0.9676 - auc: 0.9940\n",
      "Epoch 13: val_auc did not improve from 0.96333\n",
      "5501/5501 [==============================] - 142s 26ms/step - loss: 0.0892 - accuracy: 0.9676 - auc: 0.9940 - val_loss: 0.3527 - val_accuracy: 0.8986 - val_auc: 0.9535\n",
      "Epoch 14/100\n",
      "5499/5501 [============================>.] - ETA: 0s - loss: 0.0695 - accuracy: 0.9756 - auc: 0.9961Restoring model weights from the end of the best epoch: 4.\n",
      "\n",
      "Epoch 14: val_auc did not improve from 0.96333\n",
      "5501/5501 [==============================] - 140s 25ms/step - loss: 0.0695 - accuracy: 0.9756 - auc: 0.9961 - val_loss: 0.4007 - val_accuracy: 0.8988 - val_auc: 0.9480\n",
      "Epoch 14: early stopping\n",
      "Training model_2...\n",
      "Epoch 1/100\n",
      "5499/5501 [============================>.] - ETA: 0s - loss: 0.4223 - accuracy: 0.8430 - auc: 0.9073\n",
      "Epoch 1: val_auc improved from -inf to 0.91449, saving model to best_model_2_01.h5\n",
      "5501/5501 [==============================] - 140s 25ms/step - loss: 0.4223 - accuracy: 0.8430 - auc: 0.9073 - val_loss: 0.4786 - val_accuracy: 0.8245 - val_auc: 0.9145\n",
      "Epoch 2/100\n",
      "5500/5501 [============================>.] - ETA: 0s - loss: 0.2940 - accuracy: 0.8841 - auc: 0.9463\n",
      "Epoch 2: val_auc improved from 0.91449 to 0.94356, saving model to best_model_2_02.h5\n",
      "5501/5501 [==============================] - 142s 26ms/step - loss: 0.2940 - accuracy: 0.8841 - auc: 0.9463 - val_loss: 0.5547 - val_accuracy: 0.7668 - val_auc: 0.9436\n",
      "Epoch 3/100\n",
      "5499/5501 [============================>.] - ETA: 0s - loss: 0.2531 - accuracy: 0.9031 - auc: 0.9597\n",
      "Epoch 3: val_auc did not improve from 0.94356\n",
      "5501/5501 [==============================] - 134s 24ms/step - loss: 0.2531 - accuracy: 0.9031 - auc: 0.9597 - val_loss: 3.1477 - val_accuracy: 0.7522 - val_auc: 0.6794\n",
      "Epoch 4/100\n",
      "5501/5501 [==============================] - ETA: 0s - loss: 0.2277 - accuracy: 0.9145 - auc: 0.9672\n",
      "Epoch 4: val_auc did not improve from 0.94356\n",
      "5501/5501 [==============================] - 138s 25ms/step - loss: 0.2277 - accuracy: 0.9145 - auc: 0.9672 - val_loss: 1.3418 - val_accuracy: 0.8212 - val_auc: 0.8606\n",
      "Epoch 5/100\n",
      "5499/5501 [============================>.] - ETA: 0s - loss: 0.2071 - accuracy: 0.9227 - auc: 0.9726\n",
      "Epoch 5: val_auc did not improve from 0.94356\n",
      "5501/5501 [==============================] - 138s 25ms/step - loss: 0.2071 - accuracy: 0.9227 - auc: 0.9726 - val_loss: 0.5204 - val_accuracy: 0.8058 - val_auc: 0.9373\n",
      "Epoch 6/100\n",
      "5500/5501 [============================>.] - ETA: 0s - loss: 0.1919 - accuracy: 0.9286 - auc: 0.9763\n",
      "Epoch 6: val_auc improved from 0.94356 to 0.97303, saving model to best_model_2_06.h5\n",
      "5501/5501 [==============================] - 139s 25ms/step - loss: 0.1919 - accuracy: 0.9286 - auc: 0.9763 - val_loss: 0.2169 - val_accuracy: 0.9243 - val_auc: 0.9730\n",
      "Epoch 7/100\n",
      "5500/5501 [============================>.] - ETA: 0s - loss: 0.1797 - accuracy: 0.9340 - auc: 0.9790\n",
      "Epoch 7: val_auc did not improve from 0.97303\n",
      "5501/5501 [==============================] - 141s 26ms/step - loss: 0.1796 - accuracy: 0.9340 - auc: 0.9790 - val_loss: 0.2372 - val_accuracy: 0.9224 - val_auc: 0.9696\n",
      "Epoch 8/100\n",
      "5500/5501 [============================>.] - ETA: 0s - loss: 0.1674 - accuracy: 0.9386 - auc: 0.9819\n",
      "Epoch 8: val_auc did not improve from 0.97303\n",
      "5501/5501 [==============================] - 142s 26ms/step - loss: 0.1674 - accuracy: 0.9386 - auc: 0.9819 - val_loss: 0.6766 - val_accuracy: 0.8475 - val_auc: 0.8929\n",
      "Epoch 9/100\n",
      "5499/5501 [============================>.] - ETA: 0s - loss: 0.1574 - accuracy: 0.9425 - auc: 0.9837\n",
      "Epoch 9: val_auc did not improve from 0.97303\n",
      "5501/5501 [==============================] - 141s 26ms/step - loss: 0.1574 - accuracy: 0.9425 - auc: 0.9838 - val_loss: 0.3168 - val_accuracy: 0.8955 - val_auc: 0.9615\n",
      "Epoch 10/100\n",
      "5501/5501 [==============================] - ETA: 0s - loss: 0.1497 - accuracy: 0.9443 - auc: 0.9853\n",
      "Epoch 10: val_auc did not improve from 0.97303\n",
      "5501/5501 [==============================] - 158s 29ms/step - loss: 0.1497 - accuracy: 0.9443 - auc: 0.9853 - val_loss: 0.2329 - val_accuracy: 0.9125 - val_auc: 0.9666\n",
      "Epoch 11/100\n",
      "5501/5501 [==============================] - ETA: 0s - loss: 0.1401 - accuracy: 0.9488 - auc: 0.9871\n",
      "Epoch 11: val_auc improved from 0.97303 to 0.97736, saving model to best_model_2_11.h5\n",
      "5501/5501 [==============================] - 373s 68ms/step - loss: 0.1401 - accuracy: 0.9488 - auc: 0.9871 - val_loss: 0.2038 - val_accuracy: 0.9285 - val_auc: 0.9774\n",
      "Epoch 12/100\n",
      "5501/5501 [==============================] - ETA: 0s - loss: 0.1335 - accuracy: 0.9509 - auc: 0.9881\n",
      "Epoch 12: val_auc did not improve from 0.97736\n",
      "5501/5501 [==============================] - 223s 40ms/step - loss: 0.1335 - accuracy: 0.9509 - auc: 0.9881 - val_loss: 0.2937 - val_accuracy: 0.9082 - val_auc: 0.9629\n",
      "Epoch 13/100\n",
      "5500/5501 [============================>.] - ETA: 0s - loss: 0.1268 - accuracy: 0.9535 - auc: 0.9893\n",
      "Epoch 13: val_auc did not improve from 0.97736\n",
      "5501/5501 [==============================] - 145s 26ms/step - loss: 0.1268 - accuracy: 0.9535 - auc: 0.9893 - val_loss: 0.4058 - val_accuracy: 0.8825 - val_auc: 0.9509\n",
      "Epoch 14/100\n",
      "5501/5501 [==============================] - ETA: 0s - loss: 0.1201 - accuracy: 0.9554 - auc: 0.9903\n",
      "Epoch 14: val_auc did not improve from 0.97736\n",
      "5501/5501 [==============================] - 144s 26ms/step - loss: 0.1201 - accuracy: 0.9554 - auc: 0.9903 - val_loss: 6.9337 - val_accuracy: 0.7837 - val_auc: 0.7449\n",
      "Epoch 15/100\n",
      "5500/5501 [============================>.] - ETA: 0s - loss: 0.1166 - accuracy: 0.9572 - auc: 0.9908\n",
      "Epoch 15: val_auc did not improve from 0.97736\n",
      "5501/5501 [==============================] - 137s 25ms/step - loss: 0.1166 - accuracy: 0.9572 - auc: 0.9908 - val_loss: 0.2429 - val_accuracy: 0.9218 - val_auc: 0.9693\n",
      "Epoch 16/100\n",
      "5500/5501 [============================>.] - ETA: 0s - loss: 0.1120 - accuracy: 0.9588 - auc: 0.9915\n",
      "Epoch 16: val_auc did not improve from 0.97736\n",
      "5501/5501 [==============================] - 137s 25ms/step - loss: 0.1120 - accuracy: 0.9588 - auc: 0.9915 - val_loss: 0.2865 - val_accuracy: 0.9137 - val_auc: 0.9704\n",
      "Epoch 17/100\n",
      "5499/5501 [============================>.] - ETA: 0s - loss: 0.1072 - accuracy: 0.9606 - auc: 0.9920\n",
      "Epoch 17: val_auc did not improve from 0.97736\n",
      "5501/5501 [==============================] - 137s 25ms/step - loss: 0.1072 - accuracy: 0.9605 - auc: 0.9920 - val_loss: 0.2613 - val_accuracy: 0.9218 - val_auc: 0.9677\n",
      "Epoch 18/100\n",
      "5500/5501 [============================>.] - ETA: 0s - loss: 0.1048 - accuracy: 0.9621 - auc: 0.9924\n",
      "Epoch 18: val_auc did not improve from 0.97736\n",
      "5501/5501 [==============================] - 135s 25ms/step - loss: 0.1048 - accuracy: 0.9621 - auc: 0.9924 - val_loss: 0.3020 - val_accuracy: 0.9090 - val_auc: 0.9573\n",
      "Epoch 19/100\n",
      "5501/5501 [==============================] - ETA: 0s - loss: 0.1054 - accuracy: 0.9618 - auc: 0.9924\n",
      "Epoch 19: val_auc did not improve from 0.97736\n",
      "5501/5501 [==============================] - 138s 25ms/step - loss: 0.1054 - accuracy: 0.9618 - auc: 0.9924 - val_loss: 0.2806 - val_accuracy: 0.9129 - val_auc: 0.9652\n",
      "Epoch 20/100\n",
      "5501/5501 [==============================] - ETA: 0s - loss: 0.0969 - accuracy: 0.9647 - auc: 0.9934\n",
      "Epoch 20: val_auc did not improve from 0.97736\n",
      "5501/5501 [==============================] - 137s 25ms/step - loss: 0.0969 - accuracy: 0.9647 - auc: 0.9934 - val_loss: 0.2340 - val_accuracy: 0.9332 - val_auc: 0.9730\n",
      "Epoch 21/100\n",
      "5501/5501 [==============================] - ETA: 0s - loss: 0.0955 - accuracy: 0.9656 - auc: 0.9935Restoring model weights from the end of the best epoch: 11.\n",
      "\n",
      "Epoch 21: val_auc did not improve from 0.97736\n",
      "5501/5501 [==============================] - 142s 26ms/step - loss: 0.0955 - accuracy: 0.9656 - auc: 0.9935 - val_loss: 0.9639 - val_accuracy: 0.8790 - val_auc: 0.9242\n",
      "Epoch 21: early stopping\n",
      "Training model_3...\n",
      "Epoch 1/100\n",
      "5501/5501 [==============================] - ETA: 0s - loss: 0.3759 - accuracy: 0.8335 - auc: 0.9066\n",
      "Epoch 1: val_auc improved from -inf to 0.93138, saving model to best_model_3_01.h5\n",
      "5501/5501 [==============================] - 150s 27ms/step - loss: 0.3759 - accuracy: 0.8335 - auc: 0.9066 - val_loss: 0.3252 - val_accuracy: 0.8584 - val_auc: 0.9314\n",
      "Epoch 2/100\n",
      "5501/5501 [==============================] - ETA: 0s - loss: 0.3417 - accuracy: 0.8518 - auc: 0.9236\n",
      "Epoch 2: val_auc improved from 0.93138 to 0.93698, saving model to best_model_3_02.h5\n",
      "5501/5501 [==============================] - 147s 27ms/step - loss: 0.3417 - accuracy: 0.8518 - auc: 0.9236 - val_loss: 0.3129 - val_accuracy: 0.8660 - val_auc: 0.9370\n",
      "Epoch 3/100\n",
      "5500/5501 [============================>.] - ETA: 0s - loss: 0.3275 - accuracy: 0.8582 - auc: 0.9300\n",
      "Epoch 3: val_auc improved from 0.93698 to 0.93824, saving model to best_model_3_03.h5\n",
      "5501/5501 [==============================] - 150s 27ms/step - loss: 0.3276 - accuracy: 0.8582 - auc: 0.9300 - val_loss: 0.3073 - val_accuracy: 0.8673 - val_auc: 0.9382\n",
      "Epoch 4/100\n",
      "5501/5501 [==============================] - ETA: 0s - loss: 0.3174 - accuracy: 0.8642 - auc: 0.9345\n",
      "Epoch 4: val_auc improved from 0.93824 to 0.94296, saving model to best_model_3_04.h5\n",
      "5501/5501 [==============================] - 154s 28ms/step - loss: 0.3174 - accuracy: 0.8642 - auc: 0.9345 - val_loss: 0.2963 - val_accuracy: 0.8741 - val_auc: 0.9430\n",
      "Epoch 5/100\n",
      "5499/5501 [============================>.] - ETA: 0s - loss: 0.3097 - accuracy: 0.8678 - auc: 0.9379\n",
      "Epoch 5: val_auc improved from 0.94296 to 0.94334, saving model to best_model_3_05.h5\n",
      "5501/5501 [==============================] - 150s 27ms/step - loss: 0.3097 - accuracy: 0.8677 - auc: 0.9379 - val_loss: 0.2957 - val_accuracy: 0.8761 - val_auc: 0.9433\n",
      "Epoch 6/100\n",
      "5500/5501 [============================>.] - ETA: 0s - loss: 0.3017 - accuracy: 0.8718 - auc: 0.9410\n",
      "Epoch 6: val_auc improved from 0.94334 to 0.94697, saving model to best_model_3_06.h5\n",
      "5501/5501 [==============================] - 153s 28ms/step - loss: 0.3017 - accuracy: 0.8718 - auc: 0.9410 - val_loss: 0.2853 - val_accuracy: 0.8807 - val_auc: 0.9470\n",
      "Epoch 7/100\n",
      "5500/5501 [============================>.] - ETA: 0s - loss: 0.2951 - accuracy: 0.8749 - auc: 0.9437\n",
      "Epoch 7: val_auc did not improve from 0.94697\n",
      "5501/5501 [==============================] - 156s 28ms/step - loss: 0.2951 - accuracy: 0.8749 - auc: 0.9437 - val_loss: 0.2887 - val_accuracy: 0.8782 - val_auc: 0.9468\n",
      "Epoch 8/100\n",
      "5501/5501 [==============================] - ETA: 0s - loss: 0.2897 - accuracy: 0.8785 - auc: 0.9457\n",
      "Epoch 8: val_auc improved from 0.94697 to 0.94755, saving model to best_model_3_08.h5\n",
      "5501/5501 [==============================] - 151s 27ms/step - loss: 0.2897 - accuracy: 0.8785 - auc: 0.9457 - val_loss: 0.2842 - val_accuracy: 0.8806 - val_auc: 0.9476\n",
      "Epoch 9/100\n",
      "5500/5501 [============================>.] - ETA: 0s - loss: 0.2856 - accuracy: 0.8795 - auc: 0.9475\n",
      "Epoch 9: val_auc improved from 0.94755 to 0.94794, saving model to best_model_3_09.h5\n",
      "5501/5501 [==============================] - 155s 28ms/step - loss: 0.2856 - accuracy: 0.8795 - auc: 0.9475 - val_loss: 0.2842 - val_accuracy: 0.8823 - val_auc: 0.9479\n",
      "Epoch 10/100\n",
      "5500/5501 [============================>.] - ETA: 0s - loss: 0.2805 - accuracy: 0.8817 - auc: 0.9495\n",
      "Epoch 10: val_auc improved from 0.94794 to 0.94936, saving model to best_model_3_10.h5\n",
      "5501/5501 [==============================] - 150s 27ms/step - loss: 0.2805 - accuracy: 0.8817 - auc: 0.9495 - val_loss: 0.2819 - val_accuracy: 0.8839 - val_auc: 0.9494\n",
      "Epoch 11/100\n",
      "5500/5501 [============================>.] - ETA: 0s - loss: 0.2756 - accuracy: 0.8845 - auc: 0.9511\n",
      "Epoch 11: val_auc did not improve from 0.94936\n",
      "5501/5501 [==============================] - 161s 29ms/step - loss: 0.2756 - accuracy: 0.8845 - auc: 0.9511 - val_loss: 0.2823 - val_accuracy: 0.8849 - val_auc: 0.9493\n",
      "Epoch 12/100\n",
      "5501/5501 [==============================] - ETA: 0s - loss: 0.2718 - accuracy: 0.8854 - auc: 0.9525\n",
      "Epoch 12: val_auc improved from 0.94936 to 0.95085, saving model to best_model_3_12.h5\n",
      "5501/5501 [==============================] - 189s 34ms/step - loss: 0.2718 - accuracy: 0.8854 - auc: 0.9525 - val_loss: 0.2788 - val_accuracy: 0.8857 - val_auc: 0.9509\n",
      "Epoch 13/100\n",
      "5501/5501 [==============================] - ETA: 0s - loss: 0.2692 - accuracy: 0.8866 - auc: 0.9533\n",
      "Epoch 13: val_auc improved from 0.95085 to 0.95102, saving model to best_model_3_13.h5\n",
      "5501/5501 [==============================] - 173s 32ms/step - loss: 0.2692 - accuracy: 0.8866 - auc: 0.9533 - val_loss: 0.2788 - val_accuracy: 0.8864 - val_auc: 0.9510\n",
      "Epoch 14/100\n",
      "5500/5501 [============================>.] - ETA: 0s - loss: 0.2656 - accuracy: 0.8882 - auc: 0.9546\n",
      "Epoch 14: val_auc improved from 0.95102 to 0.95140, saving model to best_model_3_14.h5\n",
      "5501/5501 [==============================] - 170s 31ms/step - loss: 0.2656 - accuracy: 0.8881 - auc: 0.9546 - val_loss: 0.2763 - val_accuracy: 0.8904 - val_auc: 0.9514\n",
      "Epoch 15/100\n",
      "5499/5501 [============================>.] - ETA: 0s - loss: 0.2619 - accuracy: 0.8904 - auc: 0.9560\n",
      "Epoch 15: val_auc improved from 0.95140 to 0.95165, saving model to best_model_3_15.h5\n",
      "5501/5501 [==============================] - 168s 31ms/step - loss: 0.2619 - accuracy: 0.8904 - auc: 0.9560 - val_loss: 0.2759 - val_accuracy: 0.8884 - val_auc: 0.9517\n",
      "Epoch 16/100\n",
      "5501/5501 [==============================] - ETA: 0s - loss: 0.2584 - accuracy: 0.8921 - auc: 0.9571\n",
      "Epoch 16: val_auc did not improve from 0.95165\n",
      "5501/5501 [==============================] - 166s 30ms/step - loss: 0.2584 - accuracy: 0.8921 - auc: 0.9571 - val_loss: 0.2767 - val_accuracy: 0.8890 - val_auc: 0.9513\n",
      "Epoch 17/100\n",
      "5500/5501 [============================>.] - ETA: 0s - loss: 0.2543 - accuracy: 0.8936 - auc: 0.9584\n",
      "Epoch 17: val_auc improved from 0.95165 to 0.95208, saving model to best_model_3_17.h5\n",
      "5501/5501 [==============================] - 161s 29ms/step - loss: 0.2543 - accuracy: 0.8936 - auc: 0.9584 - val_loss: 0.2762 - val_accuracy: 0.8896 - val_auc: 0.9521\n",
      "Epoch 18/100\n",
      "5499/5501 [============================>.] - ETA: 0s - loss: 0.2519 - accuracy: 0.8948 - auc: 0.9592\n",
      "Epoch 18: val_auc did not improve from 0.95208\n",
      "5501/5501 [==============================] - 159s 29ms/step - loss: 0.2519 - accuracy: 0.8948 - auc: 0.9592 - val_loss: 0.2878 - val_accuracy: 0.8899 - val_auc: 0.9518\n",
      "Epoch 19/100\n",
      "5499/5501 [============================>.] - ETA: 0s - loss: 0.2498 - accuracy: 0.8958 - auc: 0.9600\n",
      "Epoch 19: val_auc improved from 0.95208 to 0.95257, saving model to best_model_3_19.h5\n",
      "5501/5501 [==============================] - 168s 31ms/step - loss: 0.2498 - accuracy: 0.8957 - auc: 0.9600 - val_loss: 0.2800 - val_accuracy: 0.8881 - val_auc: 0.9526\n",
      "Epoch 20/100\n",
      "5501/5501 [==============================] - ETA: 0s - loss: 0.2469 - accuracy: 0.8966 - auc: 0.9608\n",
      "Epoch 20: val_auc did not improve from 0.95257\n",
      "5501/5501 [==============================] - 161s 29ms/step - loss: 0.2469 - accuracy: 0.8966 - auc: 0.9608 - val_loss: 0.2931 - val_accuracy: 0.8858 - val_auc: 0.9509\n",
      "Epoch 21/100\n",
      "5499/5501 [============================>.] - ETA: 0s - loss: 0.2455 - accuracy: 0.8983 - auc: 0.9614\n",
      "Epoch 21: val_auc improved from 0.95257 to 0.95270, saving model to best_model_3_21.h5\n",
      "5501/5501 [==============================] - 159s 29ms/step - loss: 0.2455 - accuracy: 0.8983 - auc: 0.9614 - val_loss: 0.2794 - val_accuracy: 0.8910 - val_auc: 0.9527\n",
      "Epoch 22/100\n",
      "5500/5501 [============================>.] - ETA: 0s - loss: 0.2410 - accuracy: 0.8995 - auc: 0.9626\n",
      "Epoch 22: val_auc did not improve from 0.95270\n",
      "5501/5501 [==============================] - 161s 29ms/step - loss: 0.2410 - accuracy: 0.8995 - auc: 0.9626 - val_loss: 0.2769 - val_accuracy: 0.8914 - val_auc: 0.9525\n",
      "Epoch 23/100\n",
      "5500/5501 [============================>.] - ETA: 0s - loss: 0.2391 - accuracy: 0.9000 - auc: 0.9633\n",
      "Epoch 23: val_auc improved from 0.95270 to 0.95305, saving model to best_model_3_23.h5\n",
      "5501/5501 [==============================] - 162s 29ms/step - loss: 0.2391 - accuracy: 0.9000 - auc: 0.9633 - val_loss: 0.2763 - val_accuracy: 0.8911 - val_auc: 0.9531\n",
      "Epoch 24/100\n",
      "5501/5501 [==============================] - ETA: 0s - loss: 0.2376 - accuracy: 0.9011 - auc: 0.9638\n",
      "Epoch 24: val_auc improved from 0.95305 to 0.95385, saving model to best_model_3_24.h5\n",
      "5501/5501 [==============================] - 409s 74ms/step - loss: 0.2376 - accuracy: 0.9011 - auc: 0.9638 - val_loss: 0.2743 - val_accuracy: 0.8929 - val_auc: 0.9539\n",
      "Epoch 25/100\n",
      "5500/5501 [============================>.] - ETA: 0s - loss: 0.2343 - accuracy: 0.9018 - auc: 0.9646\n",
      "Epoch 25: val_auc did not improve from 0.95385\n",
      "5501/5501 [==============================] - 234s 43ms/step - loss: 0.2343 - accuracy: 0.9018 - auc: 0.9646 - val_loss: 0.2867 - val_accuracy: 0.8898 - val_auc: 0.9514\n",
      "Epoch 26/100\n",
      "5501/5501 [==============================] - ETA: 0s - loss: 0.2330 - accuracy: 0.9023 - auc: 0.9650\n",
      "Epoch 26: val_auc did not improve from 0.95385\n",
      "5501/5501 [==============================] - 182s 33ms/step - loss: 0.2330 - accuracy: 0.9023 - auc: 0.9650 - val_loss: 0.2818 - val_accuracy: 0.8919 - val_auc: 0.9528\n",
      "Epoch 27/100\n",
      "5499/5501 [============================>.] - ETA: 0s - loss: 0.2295 - accuracy: 0.9040 - auc: 0.9661\n",
      "Epoch 27: val_auc did not improve from 0.95385\n",
      "5501/5501 [==============================] - 178s 32ms/step - loss: 0.2295 - accuracy: 0.9040 - auc: 0.9661 - val_loss: 0.2842 - val_accuracy: 0.8915 - val_auc: 0.9530\n",
      "Epoch 28/100\n",
      "5501/5501 [==============================] - ETA: 0s - loss: 0.2290 - accuracy: 0.9040 - auc: 0.9663\n",
      "Epoch 28: val_auc did not improve from 0.95385\n",
      "5501/5501 [==============================] - 213s 39ms/step - loss: 0.2290 - accuracy: 0.9040 - auc: 0.9663 - val_loss: 0.2814 - val_accuracy: 0.8912 - val_auc: 0.9528\n",
      "Epoch 29/100\n",
      "5501/5501 [==============================] - ETA: 0s - loss: 0.2278 - accuracy: 0.9048 - auc: 0.9666\n",
      "Epoch 29: val_auc did not improve from 0.95385\n",
      "5501/5501 [==============================] - 403s 73ms/step - loss: 0.2278 - accuracy: 0.9048 - auc: 0.9666 - val_loss: 0.2891 - val_accuracy: 0.8930 - val_auc: 0.9529\n",
      "Epoch 30/100\n",
      "5500/5501 [============================>.] - ETA: 0s - loss: 0.2233 - accuracy: 0.9066 - auc: 0.9679\n",
      "Epoch 30: val_auc did not improve from 0.95385\n",
      "5501/5501 [==============================] - 174s 32ms/step - loss: 0.2233 - accuracy: 0.9066 - auc: 0.9679 - val_loss: 0.2829 - val_accuracy: 0.8930 - val_auc: 0.9536\n",
      "Epoch 31/100\n",
      "5500/5501 [============================>.] - ETA: 0s - loss: 0.2230 - accuracy: 0.9067 - auc: 0.9681\n",
      "Epoch 31: val_auc improved from 0.95385 to 0.95389, saving model to best_model_3_31.h5\n",
      "5501/5501 [==============================] - 181s 33ms/step - loss: 0.2230 - accuracy: 0.9067 - auc: 0.9680 - val_loss: 0.2794 - val_accuracy: 0.8932 - val_auc: 0.9539\n",
      "Epoch 32/100\n",
      "5500/5501 [============================>.] - ETA: 0s - loss: 0.2212 - accuracy: 0.9069 - auc: 0.9685\n",
      "Epoch 32: val_auc did not improve from 0.95389\n",
      "5501/5501 [==============================] - 178s 32ms/step - loss: 0.2212 - accuracy: 0.9069 - auc: 0.9685 - val_loss: 0.2866 - val_accuracy: 0.8937 - val_auc: 0.9530\n",
      "Epoch 33/100\n",
      "5499/5501 [============================>.] - ETA: 0s - loss: 0.2190 - accuracy: 0.9088 - auc: 0.9692\n",
      "Epoch 33: val_auc did not improve from 0.95389\n",
      "5501/5501 [==============================] - 178s 32ms/step - loss: 0.2190 - accuracy: 0.9088 - auc: 0.9692 - val_loss: 0.2858 - val_accuracy: 0.8910 - val_auc: 0.9522\n",
      "Epoch 34/100\n",
      "5499/5501 [============================>.] - ETA: 0s - loss: 0.2167 - accuracy: 0.9091 - auc: 0.9697\n",
      "Epoch 34: val_auc did not improve from 0.95389\n",
      "5501/5501 [==============================] - 167s 30ms/step - loss: 0.2167 - accuracy: 0.9091 - auc: 0.9697 - val_loss: 0.2885 - val_accuracy: 0.8937 - val_auc: 0.9530\n",
      "Epoch 35/100\n",
      "5499/5501 [============================>.] - ETA: 0s - loss: 0.2148 - accuracy: 0.9100 - auc: 0.9702\n",
      "Epoch 35: val_auc did not improve from 0.95389\n",
      "5501/5501 [==============================] - 156s 28ms/step - loss: 0.2148 - accuracy: 0.9100 - auc: 0.9702 - val_loss: 0.2884 - val_accuracy: 0.8932 - val_auc: 0.9535\n",
      "Epoch 36/100\n",
      "5500/5501 [============================>.] - ETA: 0s - loss: 0.2137 - accuracy: 0.9098 - auc: 0.9705\n",
      "Epoch 36: val_auc did not improve from 0.95389\n",
      "5501/5501 [==============================] - 177s 32ms/step - loss: 0.2137 - accuracy: 0.9098 - auc: 0.9705 - val_loss: 0.2886 - val_accuracy: 0.8931 - val_auc: 0.9525\n",
      "Epoch 37/100\n",
      "5501/5501 [==============================] - ETA: 0s - loss: 0.2132 - accuracy: 0.9105 - auc: 0.9707\n",
      "Epoch 37: val_auc did not improve from 0.95389\n",
      "5501/5501 [==============================] - 196s 36ms/step - loss: 0.2132 - accuracy: 0.9105 - auc: 0.9707 - val_loss: 0.2916 - val_accuracy: 0.8955 - val_auc: 0.9534\n",
      "Epoch 38/100\n",
      "5501/5501 [==============================] - ETA: 0s - loss: 0.2116 - accuracy: 0.9103 - auc: 0.9710\n",
      "Epoch 38: val_auc did not improve from 0.95389\n",
      "5501/5501 [==============================] - 368s 67ms/step - loss: 0.2116 - accuracy: 0.9103 - auc: 0.9710 - val_loss: 0.2890 - val_accuracy: 0.8942 - val_auc: 0.9532\n",
      "Epoch 39/100\n",
      "5499/5501 [============================>.] - ETA: 0s - loss: 0.2102 - accuracy: 0.9117 - auc: 0.9714\n",
      "Epoch 39: val_auc did not improve from 0.95389\n",
      "5501/5501 [==============================] - 192s 35ms/step - loss: 0.2102 - accuracy: 0.9116 - auc: 0.9714 - val_loss: 0.2877 - val_accuracy: 0.8949 - val_auc: 0.9537\n",
      "Epoch 40/100\n",
      "5499/5501 [============================>.] - ETA: 0s - loss: 0.2082 - accuracy: 0.9124 - auc: 0.9720\n",
      "Epoch 40: val_auc did not improve from 0.95389\n",
      "5501/5501 [==============================] - 189s 34ms/step - loss: 0.2082 - accuracy: 0.9124 - auc: 0.9720 - val_loss: 0.2961 - val_accuracy: 0.8944 - val_auc: 0.9532\n",
      "Epoch 41/100\n",
      "5499/5501 [============================>.] - ETA: 0s - loss: 0.2067 - accuracy: 0.9132 - auc: 0.9723Restoring model weights from the end of the best epoch: 31.\n",
      "\n",
      "Epoch 41: val_auc did not improve from 0.95389\n",
      "5501/5501 [==============================] - 189s 34ms/step - loss: 0.2066 - accuracy: 0.9132 - auc: 0.9723 - val_loss: 0.2929 - val_accuracy: 0.8958 - val_auc: 0.9525\n",
      "Epoch 41: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Initialize early stopping callback to monitor validation AUC and stop training when it stops improving\n",
    "early_stopping = EarlyStopping(monitor='val_auc', mode='max', patience=10, verbose=1, restore_best_weights=True)\n",
    "\n",
    "# List of models for training\n",
    "models = [model, model_2, model_3]\n",
    "\n",
    "# Train each model using a for loop, employing callbacks for early stopping and model checkpointing\n",
    "for i, model in enumerate(models):\n",
    "    print(f\"Training model_{i+1}...\")\n",
    "    \n",
    "    # Define a ModelCheckpoint callback to save the best version of each model based on validation AUC\n",
    "    model_checkpoint = ModelCheckpoint(f'best_model_{i+1}_{{epoch:02d}}.h5', monitor='val_auc', mode='max', save_best_only=True, verbose=1)\n",
    "\n",
    "    # Train the model using the fit method, passing in the data generators and callback functions\n",
    "    model.fit(\n",
    "        train_generator,\n",
    "        validation_data=validation_generator,\n",
    "        epochs=100,  # Set a high number of epochs and rely on early stopping to halt the training\n",
    "        callbacks=[early_stopping, model_checkpoint]  # List of callbacks to use during training\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results Overview\n",
    "\n",
    "Our comparative analysis of three distinct CNN architectures yielded insight regarding their performance in detecting cancer. Here's a brief summary of our findings:\n",
    "\n",
    "- **Model 1 (Custom CNN)** and **Model 2 (Enhanced CNN with Batch Normalization)** both achieved an AUC score of .97, indicating a high level of accuracy in distinguishing between cancerous and non-cancerous image patches. The performance of these two models was closely matched, with slight variations well within error margins. Notably, Model 2 reached its peak performance in just 3 epochs, compared to 9 epochs for Model 1, demonstrating the efficiency of batch normalization in stabilizing and expediting the training process.\n",
    "\n",
    "- **Model 3 (Transfer Learning with VGG16)**, while still performing well, achieved a slightly lower AUC of .95. This model required 18 epochs to converge, indicating a longer training period than the custom-built models. The results suggest that while transfer learning models have the advantage of leveraging pre-trained features, they may not always outperform models specifically designed and tuned for the task at hand.\n",
    "\n",
    "The ROC curves for each model provide a visual representation of their performance, with the area under the curve (AUC) serving as a quantifiable measure of their ability to accurately classify the images. The following plot illustrates these ROC curves and highlights the comparative performance of our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Load each model and plot the ROC curve\n",
    "for model_path, model_name in [\n",
    "    ('best_model_1_08.h5', 'Model 1'), \n",
    "    ('best_model_2_03.h5', 'Model 2'), \n",
    "    ('best_model_3_22.h5', 'Model 3')\n",
    "]:\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    # Ensure the validation generator is correctly configured for predictions\n",
    "    validation_generator.reset()\n",
    "    validation_generator.shuffle = False\n",
    "    \n",
    "    # Generate predictions and true labels\n",
    "    y_pred = model.predict(validation_generator, verbose=1)\n",
    "    y_true = validation_generator.classes  # For binary classification\n",
    "    \n",
    "    # Compute and plot ROC curve and AUC\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f'{model_name} (area = {roc_auc:.2f})')\n",
    "\n",
    "# Plot the reference line\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # Dashed diagonal\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the Sample Submission File\r\n",
    "\r\n",
    "First, we load the provided sample submission file. This file contains the IDs for each test image we need to make predictions on. Our goal is to fill in the `label` column with our model's predictions.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing Image Paths\r\n",
    "\r\n",
    "Next, we prepare the paths to the test images based on their IDs in the sample submission DataFrame. These paths will be used to load and preprocess the images before making predictions.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = [f'./test/{id}.tif' for id in submission_df['id']]\n",
    "batch_size = 32  # You can adjust the batch size according to your system's capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Image Loading and Preprocessing\r\n",
    "\r\n",
    "To handle the images efficiently, we define a function to load and preprocess image batches. This function reads images from their paths, resizes them to the target size, and normalizes their pixel value.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image_batch(image_paths, target_size=(96, 96)):\n",
    "    img_batch = np.zeros((len(image_paths), *target_size, 3), dtype='float32')\n",
    "    for i, image_path in enumerate(image_paths):\n",
    "        img = load_img(image_path, target_size=target_size)\n",
    "        img = img_to_array(img)\n",
    "        img_batch[i] = img\n",
    "    img_batch /= 255.0  # Normalizing pixel values\n",
    "    return img_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making Predictions on Test Images\r\n",
    "\r\n",
    "With our model loaded and a function in place for preprocessing, we proceed to make predictions on the test images. We process the images in batches to optimize computation and track progress using `tqdm\n",
    "\n",
    "#### Saving the Submission File\n",
    "Finally, we save our predictions to a CSV file, adhering to the competition's submission format. This file is then ready to be uploaded to Kaggle for evaluation.`.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paths = [\n",
    "    ('./best_model_1_08.h5', 'final_submission_model_1.csv'),\n",
    "    ('./best_model_2_03.h5', 'final_submission_model_2.csv'),\n",
    "    ('./best_model_3_22.h5', 'final_submission_model_3.csv'),\n",
    "]\n",
    "\n",
    "for model_path, submission_csv_path in model_paths:\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    # Predict in batches and track progress with tqdm\n",
    "    for i in tqdm(range(0, len(image_paths), batch_size), desc=f'Predicting with {model_path}'):\n",
    "        batch_paths = image_paths[i:i+batch_size]\n",
    "        img_batch = load_and_preprocess_image_batch(batch_paths)\n",
    "        batch_predictions = model.predict(img_batch, verbose=0)\n",
    "        predictions.extend(batch_predictions.flatten())\n",
    "    \n",
    "    # Create a new submission DataFrame\n",
    "    submission_df = pd.read_csv('sample_submission.csv')  # Reload to ensure clean slate\n",
    "    submission_df['label'] = predictions\n",
    "    \n",
    "    # Save the submission file for the current model\n",
    "    submission_df.to_csv(submission_csv_path, index=False)\n",
    "    print(f\"Saved submission for {model_path} to {submission_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusions\n",
    "\n",
    "Through our exploratory analysis and subsequent modeling, we've made significant progress in addressing the challenge of detecting metastatic cancer in histopathologic scan images. Our comparative analysis of three convolutional neural network architectures provided us with valuable insights:\n",
    "\n",
    "- **Model 1 (Custom CNN)** and **Model 2 (Enhanced CNN with Batch Normalization)** both achieved impressive AUC scores of .97, demonstrating their high accuracy in identifying metastatic tissue. The rapid convergence of Model 2, requiring only 3 epochs to achieve peak performance, underscores the efficiency of batch normalization in training deep learning models.\n",
    "  \n",
    "- **Model 3 (Transfer Learning with VGG16)**, while slightly trailing with an AUC of .95, showcased the potential of leveraging pre-trained networks for medical image analysis. Despite its longer convergence time, this approach remains a powerful tool for tasks with limited labeled data.\n",
    "\n",
    "#### Future Work\n",
    "\n",
    "To build on the foundations laid by this study, there are several potential areas for future work:\n",
    "\n",
    "- **Exploration of Additional Pre-trained Models**: Investigating other architectures, such as ResNet or Inception, might yield improvements in both accuracy and training efficiency for this task.\n",
    "  \n",
    "- **Advanced Data Augmentation**: Employing more sophisticated data augmentation techniques could further enhance model robustness and performance, especially in dealing with varied and complex image features.\n",
    "  \n",
    "- **Ensemble Methods**: Combining predictions from multiple models through ensemble techniques may lead to better performance by leveraging the strengths of individual models.\n",
    "\n",
    "- **Clinical Integration and Interpretability**: Beyond raw performance metrics, ensuring models are interpretable and integrate seamlessly into clinical workflows is crucial. Future work could focus on model explainability and the development of user-friendly tools for pathologists.\n",
    "\n",
    "To conclude, this project highlights the potential of deep learning in advancing medical imaging analysis. As we continue to refine our models and explore new methodologies, the prospect of significantly impacting patient care and outcomes through improved diagnostic tools becomes increasingly attractive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 862157,
     "sourceId": 11848,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30646,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
